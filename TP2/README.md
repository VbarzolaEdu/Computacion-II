# ğŸ§  TP2 â€“ Sistema de Scraping y AnÃ¡lisis Web Distribuido

## ğŸ“‹ DescripciÃ³n general

Este proyecto implementa un **sistema distribuido de scraping y anÃ¡lisis web** utilizando **Python**, **asyncio**, y **multiprocessing**.  
El sistema estÃ¡ compuesto por **dos servidores** que trabajan de forma coordinada:

- **Servidor A (Asyncio)** â†’ recibe solicitudes HTTP, realiza scraping de pÃ¡ginas web de forma asÃ­ncrona y coordina el flujo de informaciÃ³n.  
- **Servidor B (Multiprocessing)** â†’ procesa tareas pesadas en paralelo (screenshots, anÃ¡lisis de rendimiento e imÃ¡genes).  

El cliente solo interactÃºa con el **Servidor A**, que devuelve una respuesta JSON consolidada con toda la informaciÃ³n extraÃ­da y procesada.

---

## âš™ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cliente   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST /scrape
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Servidor A (Asyncio)                 â”‚
â”‚  - Recibe requests del cliente               â”‚
â”‚  - Scraping HTML asÃ­ncrono (aiohttp)         â”‚
â”‚  - ExtracciÃ³n de metadatos (BeautifulSoup)   â”‚
â”‚  - Comunica con Servidor B para tareas CPU   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Socket TCP
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Servidor B (Multiprocessing)            â”‚
â”‚  - Pool de workers (procesos)                â”‚
â”‚  - Screenshots (Playwright)                  â”‚
â”‚  - AnÃ¡lisis de rendimiento                   â”‚
â”‚  - Procesamiento de imÃ¡genes (Pillow)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flujo de trabajo:**
1. Cliente envÃ­a solicitud HTTP POST con la URL a scrapear.
2. Servidor A realiza el scraping HTML y extrae metadatos.
3. Servidor A envÃ­a tareas al Servidor B (screenshot, performance, imÃ¡genes).
4. Servidor B procesa las tareas en paralelo usando workers.
5. Servidor A recibe las respuestas del Servidor B y consolida el resultado final.
6. Servidor A devuelve al cliente un JSON con todos los datos.

---

## ğŸ§© Funcionalidades principales

| Funcionalidad | DescripciÃ³n |
|----------------|-------------|
| **Scraping HTML** | Descarga de pÃ¡ginas web usando `aiohttp` y `asyncio`. |
| **ExtracciÃ³n de metadatos** | Obtiene tÃ­tulo, meta tags, headers, links e imÃ¡genes. |
| **GeneraciÃ³n de screenshot** | Usa `Playwright` (Chromium headless) para capturar la pÃ¡gina. |
| **AnÃ¡lisis de rendimiento** | Mide tiempo de carga y tamaÃ±o total de recursos. |
| **Procesamiento de imÃ¡genes** | Descarga y genera thumbnails con `Pillow`. |
| **Manejo de errores avanzado** | Control de fallos de red, timeouts, JSON invÃ¡lido, etc. |
| **CLI configurable** | EjecuciÃ³n parametrizada con `argparse` (`-i`, `-p`, `-w`, `-n`). |
| **Testing automatizado** | Pruebas unitarias e integraciÃ³n con `pytest` y `pytest-asyncio`. |

---

## ğŸ“¦ Requisitos

### ğŸ Python
VersiÃ³n recomendada: **Python 3.10 o superior**

### ğŸ“š Dependencias
Instalar con:
```bash
pip install -r requirements.txt
```

**Paquetes incluidos:**
- `aiohttp` â†’ HTTP cliente asÃ­ncrono para scraping
- `beautifulsoup4` + `lxml` â†’ Parsing y extracciÃ³n de HTML
- `playwright` â†’ AutomatizaciÃ³n de navegadores (screenshots)
- `pillow` â†’ Procesamiento de imÃ¡genes
- `requests` â†’ HTTP cliente sÃ­ncrono
- `pytest` + `pytest-asyncio` â†’ Testing

**InstalaciÃ³n de navegadores Playwright:**
```bash
playwright install
```
Esto descarga Chromium, Firefox y WebKit. Solo se usa Chromium por defecto.

---

## ğŸš€ InstalaciÃ³n y configuraciÃ³n

### 1ï¸âƒ£ Clonar el repositorio
```bash
git clone https://github.com/VbarzolaEdu/Computacion-II.git
cd Computacion-II/TP2
```

### 2ï¸âƒ£ Crear entorno virtual (recomendado)
```bash
python3 -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate
```

### 3ï¸âƒ£ Instalar dependencias
```bash
pip install --upgrade pip
pip install -r requirements.txt
playwright install  # Descarga navegadores para Playwright
```

### 4ï¸âƒ£ Verificar instalaciÃ³n
```bash
python3 --version  # Debe ser 3.10+
pip list  # Verificar que todas las dependencias estÃ©n instaladas
```

---

## ğŸƒ EjecuciÃ³n

### Paso 1: Iniciar el Servidor B (Procesamiento)
En una terminal:
```bash
cd TP2
source .venv/bin/activate  # Si usas entorno virtual
python3 server_processing.py -i 127.0.0.1 -p 9000 -n 4
```

**ParÃ¡metros:**
- `-i` / `--ip`: IP del servidor (por defecto: `127.0.0.1`)
- `-p` / `--port`: Puerto TCP (por defecto: `9000`)
- `-n` / `--num-workers`: NÃºmero de workers/procesos (por defecto: `4`)

**Salida esperada:**
```
[INFO] Servidor de procesamiento iniciado en 127.0.0.1:9000
[INFO] Pool de 4 workers creado
```

### Paso 2: Iniciar el Servidor A (Scraping)
En otra terminal:
```bash
cd TP2
source .venv/bin/activate
python3 server_scraping.py -i 127.0.0.1 -p 8000 -w 4
```

**ParÃ¡metros:**
- `-i` / `--ip`: IP del servidor HTTP (por defecto: `127.0.0.1`)
- `-p` / `--port`: Puerto HTTP (por defecto: `8000`)
- `-w` / `--workers`: NÃºmero de workers asyncio (por defecto: `4`)

**Salida esperada:**
```
[INFO] Servidor de scraping iniciado en http://127.0.0.1:8000
[INFO] Conectado al servidor de procesamiento en 127.0.0.1:9000
```

### Paso 3: Ejecutar el cliente
En una tercera terminal:
```bash
cd TP2
source .venv/bin/activate
python3 client.py
```

**Ejemplo de uso interactivo:**
```
Ingrese la URL a scrapear: https://example.com
```

**Respuesta JSON (ejemplo simplificado):**
```json
{
  "url": "https://example.com",
  "title": "Example Domain",
  "meta": {
    "description": "Example domain for illustrative examples",
    "charset": "UTF-8"
  },
  "headers": {
    "content-type": "text/html",
    "server": "nginx"
  },
  "links": ["https://www.iana.org/domains/example"],
  "images": ["https://example.com/image.png"],
  "screenshot": "iVBORw0KGgoAAAANSUhEUg...",
  "performance": {
    "load_time_ms": 234,
    "total_size_kb": 1270,
    "num_requests": 8
  },
  "thumbnails": ["data:image/png;base64,..."]
}
```

---

## ğŸ§ª Testing

### Ejecutar todos los tests
```bash
cd TP2
source .venv/bin/activate  # Si usas entorno virtual
python -m pytest -v
```

### Ejecutar tests especÃ­ficos
```bash
# Solo tests del procesador
python -m pytest -v tests/test_processor.py

# Solo tests del scraper
python -m pytest -v tests/test_scraper.py

# Test especÃ­fico
python -m pytest -v tests/test_processor.py::test_take_screenshot_returns_base64
```

### Cobertura de tests (opcional)
```bash
pip install pytest-cov
python -m pytest --cov=. --cov-report=html
```
Los resultados se generan en `htmlcov/index.html`.

**Tests incluidos:**
- âœ… ValidaciÃ³n de estructura de respuestas
- âœ… Manejo de errores (timeouts, URLs invÃ¡lidas)
- âœ… Procesamiento de imÃ¡genes
- âœ… Screenshots con Playwright
- âœ… AnÃ¡lisis de performance

---

## ğŸ“ Estructura del proyecto

```
TP2/
â”œâ”€â”€ client.py                    # Cliente HTTP para probar el sistema
â”œâ”€â”€ server_scraping.py           # Servidor A (asyncio + HTTP)
â”œâ”€â”€ server_processing.py         # Servidor B (multiprocessing)
â”œâ”€â”€ requirements.txt             # Dependencias del proyecto
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ .gitignore                   # Archivos ignorados por git
â”‚
â”œâ”€â”€ common/                      # MÃ³dulos compartidos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ protocol.py              # Protocolo de comunicaciÃ³n entre servidores
â”‚   â””â”€â”€ serialization.py         # SerializaciÃ³n/deserializaciÃ³n de mensajes
â”‚
â”œâ”€â”€ scraper/                     # MÃ³dulos de scraping
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ async_http.py            # Cliente HTTP asÃ­ncrono
â”‚   â”œâ”€â”€ html_parser.py           # Parser HTML con BeautifulSoup
â”‚   â””â”€â”€ metadata_extractor.py   # ExtracciÃ³n de metadatos
â”‚
â”œâ”€â”€ processor/                   # MÃ³dulos de procesamiento
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ screenshot.py            # GeneraciÃ³n de screenshots (Playwright)
â”‚   â”œâ”€â”€ performance.py           # AnÃ¡lisis de rendimiento
â”‚   â””â”€â”€ image_processor.py       # Procesamiento de imÃ¡genes (thumbnails)
â”‚
â”œâ”€â”€ tests/                       # Tests automatizados
â”‚   â”œâ”€â”€ test_scraper.py          # Tests del mÃ³dulo scraper
â”‚   â””â”€â”€ test_processor.py        # Tests del mÃ³dulo processor
â”‚
â””â”€â”€ venv/                        # Entorno virtual (no trackeado en git)
```

---

## ğŸ”§ ConfiguraciÃ³n avanzada

### Variables de entorno (opcional)
Puedes crear un archivo `.env` para configurar parÃ¡metros por defecto:
```bash
# .env
SCRAPER_HOST=0.0.0.0
SCRAPER_PORT=8000
PROCESSOR_HOST=127.0.0.1
PROCESSOR_PORT=9000
NUM_WORKERS=4
```

### Aumentar lÃ­mite de workers
Para sitios con muchas imÃ¡genes/recursos:
```bash
python3 server_processing.py -n 8  # Aumentar a 8 workers
```

### Timeout personalizado
Editar constantes en `scraper/async_http.py` o `processor/screenshot.py`:
```python
TIMEOUT = 30  # segundos
```

---

## ğŸ› Troubleshooting

### Error: `ModuleNotFoundError: No module named 'processor'`
**SoluciÃ³n:** Ejecutar desde el directorio `TP2/`:
```bash
cd TP2
python3 server_scraping.py
```

### Error: `playwright: command not found` o `Browser not found`
**SoluciÃ³n:** Instalar navegadores de Playwright:
```bash
playwright install
```

### Error: `Address already in use`
**SoluciÃ³n:** El puerto estÃ¡ ocupado. Cambia el puerto o mata el proceso:
```bash
# Ver procesos usando el puerto 8000
lsof -i :8000
# Matar el proceso
kill -9 <PID>
```

### Tests fallan con timeout
**SoluciÃ³n:** Algunos tests requieren conexiÃ³n a internet. Verifica tu red o aumenta el timeout en los tests.

### `pip install -r requirements.txt` falla
**SoluciÃ³n:** AsegÃºrate de que `requirements.txt` no contenga lÃ­neas invÃ¡lidas como `common.errors` (ya fue corregido).

---

## ğŸ“ Notas adicionales

- **Desactivar entorno virtual:** `deactivate`
- **Actualizar dependencias:** `pip install --upgrade -r requirements.txt`
- **Limpiar cachÃ© de Python:** `find . -type d -name __pycache__ -exec rm -rf {} +`
- **Git ignore:** El archivo `.gitignore` ya excluye `.venv/` y `__pycache__/`

---

## ğŸ“š Recursos y referencias

- [DocumentaciÃ³n de aiohttp](https://docs.aiohttp.org/)
- [BeautifulSoup4 docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Playwright Python](https://playwright.dev/python/)
- [Multiprocessing en Python](https://docs.python.org/3/library/multiprocessing.html)
- [Pytest documentation](https://docs.pytest.org/)

---

## ğŸ‘¥ Autor

**Valentin Barzola**  
Universidad de Mendoza â€“ ComputaciÃ³n II  
AÃ±o: 2025

---

## ğŸ“„ Licencia

Este proyecto es de uso acadÃ©mico y educativo.

---

